% !TEX root = developer.tex

\chapter{How \sstmacro Launches}
\label{chapter:launching}

It is useful for an intuitive understanding of the code to walk through the steps starting from \inlinecode{main} and proceeding to the discrete event simulation actually launching. The code follows these basic steps:

\begin{itemize}
\item Configuration of the simulation via environment variables, command line parameters, and the input file
\item Building and configuration of simulator components
\item Running of the actual simulation
\end{itemize}

We can walk through each of these steps in more detail.

\section{Configuration of Simulation}\label{sec:simConfig}
The configuration proceeds through the following basic steps:
\begin{itemize}
\item Basic initialization of the \inlinecode{parallel_runtime} object from environment variables and command line parameters
\item Processing and parallel broadcast of the input file parameters
\item Creation of the simulation \inlinecode{manager} object
\item Detailed configuration of the \inlinecode{manager} and \inlinecode{parallel_runtime} object
\end{itemize}

The first step in most programs is to initialize the parallel communication environment via calls to MPI\_Init or similar.
Only rank 0 should read in the input file to minimize filesystem traffic in a parallel job.
Rank 0 then broadcasts the parameters to all other ranks.
We are thus left with the problem of wanting to tune initialization of the parallel environment via the input file,
but the input file is not yet available.
Thus, we have an initial bootstrap step where the all parameters affecting initialization of the parallel runtime must be given
either via command line parameters or environment variables.
These automatically get distributed to all processes via the job launcher.
Most critically the environment variable \inlineshell{SSTMC_PARALLEL} takes on values of \inlineshell{serial} or \inlineshell{mpi}.

As stated above, only rank 0 ever touches the filesystem.
A utility is provided within the Sprockit library for automatically distributing files via the \inlinecode{parallel_build_params} function within \inlinecode{sim_parameters}.
Once broadcast, all ranks now have all they need to configure, setup, and run.
Some additional processing is done here to map parameters.
If parameters are missing, \sstmacro may fill in sensible defaults at this stage.
For deprecated parameters, \sstmacro also does some remapping to ensure backwards compatibility.

After creation of the \inlinecode{manager} object, 
since all of the parameters even from the input file are now available,
a more detailed configuration of the \inlinecode{manager} and \inlinecode{parallel_runtime} can be done.

\section{Building and configuration of simulator components}\label{sec:buildConfig}
Inside the constructor for \inlinecode{manager},
the simulation manager now proceeds to build all the necessary components.
There are three important components to build.

\begin{itemize}
\item The event manager that drives the discrete event simulation
\item The interconnect object that directs the creation of all the hardware components
\item The generation of application objects that will drive the software events. This is built indirectly through node objects that are built by the interconnect.
\end{itemize}

\subsection{Event Manager}\label{sec:eventMan}
The \inlinecode{event_manager} object is a polymorphic type that depends on 1) what sort of parallelism is being used and 2) what sort of data structure is being used.
Some allowed values include \inlineshell{event_map} or \inlineshell{event_calendar} via the \inlineshell{event_manager} variable in the input file.
For parallel simulation, only the \inlineshell{event_map} data structure is currently supported.
For MPI parallel simulations, the \inlineshell{event_manager} parameter should be set to \inlineshell{clock_cycle_parallel}.
For multithreaded simulations (single process or coupled with MPI), this should be set to \inlineshell{multithread}.
In most cases, \sstmacro chooses a sensible default based on the configuration and installation.

As of right now, the event manager is also responsible for partitioning the simulation.
This may be refactored in future versions.
This creates something of a circular dependency between the \inlinecode{event_manager} and the \inlinecode{interconnect} objects.
When scheduling events and sending events remotely,
it is highly convenient to have the partition information accessible by the event manager.
For now, the event manager reads the topology information from the input file.
It then determines the total number of hardware components and does the partitioning.
This partitioning object is passed on to the interconnect.

\subsection{Interconnect}\label{subsec:interconnect}
The interconnect is the workhorse for building all hardware components.
After receiving the partition information from the \inlinecode{event_manager},
the interconnect creates all the nodes, switches, and NICs the current MPI rank is responsible for.
In parallel runs, each MPI rank only gets assigned a unique, disjoint subset of the components.
The interconnect then also creates all the connections between components that are linked based on the topology input (see Section \ref{sec:connectables}).
For components that are not owned by the current MPI rank, the interconnect inserts a dummy handler that informs the \inlinecode{event_manager}
that the message needs to be re-routed to another MPI rank.

\subsection{Applications}\label{subsec:apps}
All events generated in the simulation ultimately originate from application objects.
All hardware events start from real application code.
The interconnect builds a set of node objects corresponding to compute nodes in the system.
In the constructor for \inlinecode{node} we have:

\begin{CppCode}
job_launcher_ = job_launcher::static_job_launcher(params, mgr;
\end{CppCode}

This job launcher roughly corresponds to SLURM, PBS, or MOAB - some process manager that will allocate nodes to a job request and spawn processes on the nodes. For implementation reasons, each node grabs a reference to a static job launcher.  After construction, each node will have its \inlinecode{init} function invoked.

\begin{CppCode}
void
node::init(unsigned int phase)
{
  if (phase == 0){
    build_launchers(params_);
  }
}
\end{CppCode}
The \inlinecode{build_launchers} will detect all the launch requests from the input file.  After the init phases are completed, a final setup function is invoked on the node.

\begin{CppCode}
void
node::schedule_launches()
{
  for (app_launch* appman : launchers_){
    schedule(appman->time(), new_callback(this, &node::job_launch, appman));
  }
}
\end{CppCode}
The function \inlinecode{appman->time()} returns the time that the application launch is \emph{requested}, not when the application  necessarily launches.
This corresponds to when a user would type, e.g. \inlinecode{srun} or \inlinecode{qsub} to put the job in a queue.
When the time for a job launch request is reached, the callback function is invoked.

\begin{CppCode}
void
node::job_launch(app_launch* appman)
{
  job_launcher_->handle_new_launch_request(appman, this);
}
\end{CppCode}
For the default job launcher (in most cases SST/macro only simulates a single job in which case no scheduler is needed) the job launches immediately. 
The code for the default job launcher is:

\begin{CppCode}
ordered_node_set allocation;
appman->request_allocation(available_, allocation);
for (const node_id& nid : allocation){
  if (available_.find(nid) == available_.end()){
    spkt_throw_printf(sprockit::value_error,
                      "allocation requested node %d, but it's not available",
                      int(nid));
  }
  available_.erase(nid);
}
appman->index_allocation(allocation);

for (int& rank : appman->rank_assignment(nd->addr()){
  sw::launch_event* lev = new launch_event(appman->app_template(), appman->aid(),
                                             rank, appman->core_affinities());
  nd->handle(lev);
}
\end{CppCode}
Here the application manager first allocates the correct number of nodes and indexes (assigns task numbers to nodes).
This is detailed in the user's manual.
The application manager has a launch info object that contains all the information needed to launch a new instance of the application on each node.
The application manager then loops through all processes it is supposed to launch,
queries for the correct node assignment,
and fetches the physical node that will launch the application.

Every application gets assigned a \inlinecode{software_id}, which is a struct containing a \inlinecode{task_id} and \inlinecode{app_id}.
The task ID identifies the process number (essentially MPI rank). 
The application ID identifies which currently running application instance is being used.
This is only relevant where two distinct applications are running.
In most cases, only a single application is being used, in which case the application ID is always one.


\section{Running}\label{sec:running}
Now that all hardware components have been created and all application objects have been assigned to physical nodes,
the \inlinecode{event_manager} created above is started.
It begins looping through all events in the queue ordered by timestamp and runs them.
As stated above, all events originate from application code.
Thus, the first events to run are always the application launch events generated from the launch messages sent to the nodes generated the job launcher.

